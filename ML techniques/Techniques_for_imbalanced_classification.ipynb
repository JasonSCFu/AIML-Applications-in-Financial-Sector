{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction**\n",
        "\n",
        "Classification predictive modeling involves predicting a class label for a given observation.\n",
        "\n",
        "An imbalanced classification problem is an example of a classification problem where the distribution of examples across the known classes is biased or skewed. The distribution can vary from a slight bias to a severe imbalance where there is one example in the minority class for hundreds, thousands, or millions of examples in the majority class or classes.\n",
        "\n",
        "Imbalanced classifications pose a challenge for predictive modeling as most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class. This results in models that have poor predictive performance, specifically for the minority class. This is a problem because typically, the minority class is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class.\n",
        "\n",
        "In the world of banking analyltics, imbalanced classification are commonly seen in the areas of:\n",
        "\n",
        "\n",
        "*   Fraud Detection\n",
        "*   Claim Prediction\n",
        "*   Default Prediction\n",
        "*   Churn Prediction\n",
        "*   Spam Detection\n",
        "*   Anomaly Detection\n",
        "*   Outlier Detection\n",
        "*   Intrusion Detection\n",
        "*   Conversion Prediction\n"
      ],
      "metadata": {
        "id": "MElyBeMtAGtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Tactics To Combat Imbalanced Training Data**\n",
        "\n",
        "1. Can you collect more data?\n",
        "2. Try changing your performance metric:\n",
        "> *   Confusion matrix\n",
        "> *   Precision: A measure of a classifiers exactness\n",
        "> *   Recall: A measure of a classifiers completeness\n",
        "> *   F1 Score (or F-score): A weighted average of precision and recall\n",
        "> *   Kappa (or Cohenâ€™s kappa): Classification accuracy normalized by the imbalance of the classes in the data\n",
        "> *   ROC Curves: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values\n",
        "\n",
        "3. Try resampling your dataset\n",
        "4. Try generate synthetic samples\n",
        "5. Try penalized models or add weights/cost for minor cases\n",
        "6. Try a different perspective: take the example of Anomaly Detection, we can consider the minor class as the outliers class, hence we might use unsupervised learning to detect the outliers.\n",
        ">     \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N82yE2xF7W3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will further demo how to use the SMOTE and Near Miss Algorithm in the following section."
      ],
      "metadata": {
        "id": "K1DftIK48KCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary modules \n",
        "import pandas  as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "uZQDc1BzUwxT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**\n",
        "I will use the [Kaggle credit card dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) demonstrate the techniques.\n",
        "The dataset consists of transactions made by credit cards. This dataset has 492 fraud transactions out of 284, 807 transactions. That makes it highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n"
      ],
      "metadata": {
        "id": "QtLUskB6O7rk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C935Tzo6_RQL",
        "outputId": "b25411a4-bf40-4106-c49d-f465f45b8850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# load the data set\n",
        "data = pd.read_csv('/content/creditcard.csv')\n",
        "  \n",
        "# print info about columns in the dataframe\n",
        "print(data.info())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalise the amount column\n",
        "data['normAmount'] = StandardScaler().fit_transform(np.array(data['Amount']).reshape(-1, 1))\n",
        "  \n",
        "# drop Time and Amount columns as they are not relevant for prediction purpose \n",
        "data = data.drop(['Time', 'Amount'], axis = 1)\n",
        "  \n",
        "# as you can see there are 492 fraud transactions.\n",
        "data['Class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeUPjGnfDRpo",
        "outputId": "a24bf730-ef65-4344-af9c-44cc681dae7f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    284315\n",
              "1       492\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X = data.loc[:, data.columns != \"Class\"]\n",
        "y = data['Class']\n",
        "# split into 70:30 ration\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
        "  \n",
        "# describes info about train and test set\n",
        "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
        "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
        "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
        "print(\"Number transactions y_test dataset: \", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t5ruvvRDXk9",
        "outputId": "cfa6fc5f-99d4-470e-e171-a3dd2982cc40"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number transactions X_train dataset:  (199364, 29)\n",
            "Number transactions y_train dataset:  (199364,)\n",
            "Number transactions X_test dataset:  (85443, 29)\n",
            "Number transactions y_test dataset:  (85443,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit a naive logistic regression to it and later we will compare the model result with the SMOTE and Near Miss method."
      ],
      "metadata": {
        "id": "tOwwMrtTeAmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logistic regression object\n",
        "lr = LogisticRegression()\n",
        "  \n",
        "# train the model on train set\n",
        "lr.fit(X_train, y_train.ravel())\n",
        "  \n",
        "predictions = lr.predict(X_test)\n",
        "  \n",
        "# print classification report\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEvqq2Djd7JM",
        "outputId": "c77fdf0a-9228-4317-acb5-2e2f071584bd"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     85296\n",
            "           1       0.88      0.62      0.73       147\n",
            "\n",
            "    accuracy                           1.00     85443\n",
            "   macro avg       0.94      0.81      0.86     85443\n",
            "weighted avg       1.00      1.00      1.00     85443\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy comes out to be 100%, the recall of the minority class in very less. It proves that the model is more biased towards majority class. So, it proves that this is not the best model.\n",
        "Now, we will apply different imbalanced data handling techniques and see their accuracy and recall results."
      ],
      "metadata": {
        "id": "8exLFtF3e6_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Synthetic Minority Oversampling Technique(SMOTE)**\n",
        "\n",
        "One way to solve this problem is to oversample the examples in the minority class. This can be achieved by simply duplicating examples from the minority class in the training dataset prior to fitting a model. This can balance the class distribution but does not provide any additional information to the model.\n",
        "\n",
        "An improvement on duplicating examples from the minority class is to synthesize new examples from the minority class. This is a type of data augmentation for tabular data and can be very effective.\n",
        "\n",
        "SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n",
        "\n",
        "Specifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space."
      ],
      "metadata": {
        "id": "Uw8_JcPDCZza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "O5DKrN7VQ2Kw"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
        "\n",
        "sm = SMOTE(random_state = 2)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n",
        "  \n",
        "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
        "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
        "  \n",
        "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1)))\n",
        "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zP4KZhIxD1jL",
        "outputId": "8e6346e2-c4ef-449d-94be-b1e596ff11ef"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before OverSampling, counts of label '1': 345\n",
            "Before OverSampling, counts of label '0': 199019 \n",
            "\n",
            "After OverSampling, the shape of train_X: (398038, 29)\n",
            "After OverSampling, the shape of train_y: (398038,) \n",
            "\n",
            "After OverSampling, counts of label '1': 199019\n",
            "After OverSampling, counts of label '0': 199019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr1 = LogisticRegression()\n",
        "lr1.fit(X_train_res, y_train_res.ravel())\n",
        "predictions = lr1.predict(X_test)\n",
        "  \n",
        "# print classification report\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qASLwLfJeNQb",
        "outputId": "fef43cc4-9cab-4812-d0ac-b53e799788a9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     85296\n",
            "           1       0.06      0.92      0.11       147\n",
            "\n",
            "    accuracy                           0.98     85443\n",
            "   macro avg       0.53      0.95      0.55     85443\n",
            "weighted avg       1.00      0.98      0.99     85443\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have reduced the accuracy to 98% as compared to previous model but the recall value of minority class has also improved to 92 %. This is a good model compared to the previous one. Recall is great.\n",
        "\n",
        "Now, we will apply NearMiss technique to Under-sample the majority class and see its accuracy and recall results."
      ],
      "metadata": {
        "id": "iZDKkm7hehHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **NearMiss Algorithm**\n",
        "\n",
        "NearMiss is an under-sampling technique. It aims to balance class distribution by randomly eliminating majority class examples. When instances of two different classes are very close to each other, we remove the instances of the majority class to increase the spaces between the two classes. This helps in the classification process.\n",
        "To prevent problem of information loss in most under-sampling techniques, near-neighbor methods are widely used.\n",
        "The basic intuition about the working of near-neighbor methods is as follows:\n",
        "\n",
        "*   Step 1: The method first finds the distances between all instances of the majority class and the instances of the minority class. Here, majority class is to be under-sampled.\n",
        "*   Step 2: Then, n instances of the majority class that have the smallest distances to those in the minority class are selected.\n",
        "*   Step 3: If there are k instances in the minority class, the nearest method will result in k*n instances of the majority class.\n",
        "For finding n closest instances in the majority class, there are several variations of applying NearMiss Algorithm :\n",
        "\n",
        "\n",
        "\n",
        "> *    NearMiss â€“ Version 1 : It selects samples of the majority class for which average distances to the k closest instances of the minority class is smallest.\n",
        "\n",
        "\n",
        "> *    NearMiss â€“ Version 2 : It selects samples of the majority class for which average distances to the k farthest instances of the minority class is smallest.\n",
        "\n",
        "> *    NearMiss â€“ Version 3 : It works in 2 steps. Firstly, for each minority class instance, their M nearest-neighbors will be stored. Then finally, the majority class instances are selected for which the average distance to the N nearest-neighbors is the largest."
      ],
      "metadata": {
        "id": "qQu5PLgSbiOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
        "  \n",
        "# apply near miss\n",
        "from imblearn.under_sampling import NearMiss\n",
        "nr = NearMiss()\n",
        "  \n",
        "X_train_miss, y_train_miss = nr.fit_resample(X_train, y_train.ravel())\n",
        "  \n",
        "print('After Undersampling, the shape of train_X: {}'.format(X_train_miss.shape))\n",
        "print('After Undersampling, the shape of train_y: {} \\n'.format(y_train_miss.shape))\n",
        "  \n",
        "print(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_miss == 1)))\n",
        "print(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_miss == 0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bouubj_ESmV",
        "outputId": "bd59d9b7-fc76-4073-e4e1-bc85c62c1dbc"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Undersampling, counts of label '1': 345\n",
            "Before Undersampling, counts of label '0': 199019 \n",
            "\n",
            "After Undersampling, the shape of train_X: (690, 29)\n",
            "After Undersampling, the shape of train_y: (690,) \n",
            "\n",
            "After Undersampling, counts of label '1': 345\n",
            "After Undersampling, counts of label '0': 345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NearMiss Algorithm has undersampled the majority instances and made it equal to majority class. Here, the majority class has been reduced to the total number of minority class, so that both classes will have equal number of records."
      ],
      "metadata": {
        "id": "3dkZQL-kegFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model on train set\n",
        "lr2 = LogisticRegression()\n",
        "lr2.fit(X_train_miss, y_train_miss.ravel())\n",
        "predictions = lr2.predict(X_test)\n",
        "  \n",
        "# print classification report\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaZB_XQpeZ8z",
        "outputId": "e858472e-a544-40bf-b5d3-45dcc2f99d21"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.55      0.71     85296\n",
            "           1       0.00      0.95      0.01       147\n",
            "\n",
            "    accuracy                           0.56     85443\n",
            "   macro avg       0.50      0.75      0.36     85443\n",
            "weighted avg       1.00      0.56      0.71     85443\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model is better than the first model because it classifies better and also the recall value of minority class is 95 %. But due to undersampling of majority class, its recall has decreased to 56 %. So in this case, SMOTE is giving me a great accuracy and recall."
      ],
      "metadata": {
        "id": "1cvpe1Pxqp8-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "csvyiyMnqZeN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}